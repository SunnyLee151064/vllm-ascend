# Adapt from https://github.com/fla-org/flash-linear-attention/blob/main/fla/modules/layernorm_gated.py
# Copyright (c) 2024, Tri Dao.
# Based on the Triton LayerNorm tutorial: https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html
# For the backward pass, we keep weight_grad and bias_grad in registers and accumulate.
# This backward pass is faster for dimensions up to 8k, but after that it's much slower due to register spilling.
# The models we train have hidden dim up to 8k anyway (e.g. Llama 70B), so this is fine.
# mypy: ignore-errors

import torch
import torch.nn.functional as F
from vllm.triton_utils import tl, triton
from typing import Optional
import triton.runtime.driver as driver
USE_DEFAULT_FLA_NORM = False

MAX_CORES = 65535


# @triton.heuristics({
#     "HAS_BIAS": lambda args: args["B"] is not None,
#     "HAS_Z": lambda args: args["Z"] is not None,
# })
# @triton.jit
# def layer_norm_fwd_kernel(
#     X,  # pointer to the input
#     Y,  # pointer to the output
#     W,  # pointer to the weights
#     B,  # pointer to the biases
#     Z,  # pointer to the other branch
#     Mean,  # pointer to the mean
#     Rstd,  # pointer to the 1/std
#     stride_x_row,  # how much to increase the pointer when moving by 1 row
#     stride_y_row,
#     stride_z_row,
#     M,  # number of rows in X_base
#     N,  # number of columns in X_base
#     eps,  # epsilon to avoid division by zero
#     BLOCK_N: tl.constexpr,
#     HAS_BIAS: tl.constexpr,
#     HAS_Z: tl.constexpr,
#     NORM_BEFORE_GATE: tl.constexpr,
#     IS_RMS_NORM: tl.constexpr,
#     N_CORES: tl.constexpr,
# ):
#     # Map the program id to the row of X_base and Y_base it should compute.
#     row = tl.program_id(0)
#     group = tl.program_id(1)

#     BLOCK_ROWS = M if M < N_CORES else N_CORES
#     n_iters = M // BLOCK_ROWS
#     remain = M % BLOCK_ROWS
#     if row < remain:
#         n_iters = n_iters + 1

#     for i in tl.range(n_iters):
#         X_base = X + (i * BLOCK_ROWS *
#                       stride_x_row) + row * stride_x_row + group * N
#         Y_base = Y + (i * BLOCK_ROWS *
#                       stride_y_row) + row * stride_y_row + group * N
#         if HAS_Z:
#             Z_base = Z + (i * BLOCK_ROWS *
#                           stride_z_row) + row * stride_z_row + group * N
#         if not IS_RMS_NORM:
#             Mean_base = Mean + (i * BLOCK_ROWS) + group * M
#         Rstd_base = Rstd + (i * BLOCK_ROWS) + group * M
#         W_base = W + group * N
#         if HAS_BIAS:
#             B_base = B + group * N
#         # Compute mean and variance
#         cols = tl.arange(0, BLOCK_N)
#         x = tl.load(X_base + cols, mask=cols < N, other=0.).to(tl.float32)
#         if HAS_Z and not NORM_BEFORE_GATE:
#             z = tl.load(Z_base + cols, mask=cols < N).to(tl.float32)
#             x *= z * tl.sigmoid(z)
#         if not IS_RMS_NORM:
#             mean = tl.sum(x, axis=0) / N
#             tl.store(Mean_base + row, mean)
#             xbar = tl.where(cols < N, x - mean, 0.)
#             var = tl.sum(xbar * xbar, axis=0) / N
#         else:
#             xbar = tl.where(cols < N, x, 0.)
#             var = tl.sum(xbar * xbar, axis=0) / N
#         rstd = 1 / tl.sqrt(var + eps)
#         tl.store(Rstd_base + row, rstd)
#         # Normalize and apply linear transformation
#         mask = cols < N
#         w = tl.load(W_base + cols, mask=mask).to(tl.float32)
#         if HAS_BIAS:
#             b = tl.load(B_base + cols, mask=mask).to(tl.float32)
#         x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd
#         y = x_hat * w + b if HAS_BIAS else x_hat * w
#         if HAS_Z and NORM_BEFORE_GATE:
#             z = tl.load(Z_base + cols, mask=mask).to(tl.float32)
#             y *= z * tl.sigmoid(z)
#         # Write output
#         tl.store(Y_base + cols, y, mask=mask)


def _layer_norm_fwd(
    x,
    weight,
    bias,
    eps,
    z=None,
    out=None,
    group_size=None,
    norm_before_gate=True,
    is_rms_norm=False,
):
    M, N = x.shape
    if group_size is None:
        group_size = N
    assert N % group_size == 0
    ngroups = N // group_size
    assert x.stride(-1) == 1
    if z is not None:
        assert z.stride(-1) == 1
        assert z.shape == (M, N)
    assert weight.shape == (N, )
    assert weight.stride(-1) == 1
    if bias is not None:
        assert bias.stride(-1) == 1
        assert bias.shape == (N, )
    # allocate output
    if out is not None:
        assert out.shape == x.shape
    else:
        out = torch.empty_like(x)
    assert out.stride(-1) == 1
    mean = (torch.empty((ngroups * M, ), dtype=torch.float32, device=x.device)
            if not is_rms_norm else None)
    rstd = torch.empty((ngroups * M, ), dtype=torch.float32, device=x.device)
    # Less than 64KB per feature: enqueue fused kernel
    MAX_FUSED_SIZE = 65536 // x.element_size()
    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(group_size))
    if group_size > BLOCK_N:
        raise RuntimeError(
            "This layer norm doesn't support feature dim >= 64KB.")
    # heuristics for number of warps
    num_warps = min(max(BLOCK_N // 256, 1), 8)
    grid = (M if M < MAX_CORES else MAX_CORES, ngroups)
    with torch.npu.device(x.device.index):
        layer_norm_fwd_kernel[grid](
            x,
            out,
            weight,
            bias,
            z,
            mean,
            rstd,
            x.stride(0),
            out.stride(0),
            z.stride(0) if z is not None else 0,
            M,
            group_size,
            eps,
            BLOCK_N=BLOCK_N,
            NORM_BEFORE_GATE=norm_before_gate,
            IS_RMS_NORM=is_rms_norm,
            N_CORES=MAX_CORES,
            num_warps=num_warps,
        )
    return out, mean, rstd


# class LayerNormFn(torch.autograd.Function):

#     @staticmethod
#     def forward(
#         ctx,
#         x,
#         weight,
#         bias,
#         z=None,
#         eps=1e-6,
#         group_size=None,
#         norm_before_gate=True,
#         is_rms_norm=False,
#     ):
#         """If z is not None, we do norm(x) * silu(z) if norm_before_gate, else norm(x * silu(z))"""

#         x_shape_og = x.shape
#         # reshape input data into 2D tensor
#         x = x.reshape(-1, x.shape[-1])
#         if x.stride(-1) != 1:
#             x = x.contiguous()
#         if z is not None:
#             assert z.shape == x_shape_og
#             z = z.reshape(-1, z.shape[-1])
#             if z.stride(-1) != 1:
#                 z = z.contiguous()
#         weight = weight.contiguous()
#         if bias is not None:
#             bias = bias.contiguous()
#         y, mean, rstd = _layer_norm_fwd(
#             x,
#             weight,
#             bias,
#             eps,
#             z=z,
#             group_size=group_size,
#             norm_before_gate=norm_before_gate,
#             is_rms_norm=is_rms_norm,
#         )
#         return y.reshape(x_shape_og)


def torch_chunk_gated_delta_rule(
    query,
    key,
    value,
    g,
    beta,
    chunk_size=64,
    initial_state=None,
    output_final_state=False,
    use_qk_l2norm_in_kernel=False,
):
    initial_dtype = query.dtype
    if use_qk_l2norm_in_kernel:
        #query = F.normalize(query, p=2, dim=-1)
        #key = F.normalize(key, p=2, dim=-1)
        query = l2norm_fwd(query)  # 算子替换
        key = l2norm_fwd(key)  # 算子替换
    query, key, value, beta, g = [
        x.transpose(1, 2).contiguous().to(torch.float32)
        for x in (query, key, value, beta, g)
    ]

    batch_size, sequence_length, num_heads, k_head_dim = key.shape
    v_head_dim = value.shape[-1]
    pad_size = (chunk_size - num_heads % chunk_size) % chunk_size
    query = F.pad(query, (0, 0, 0, pad_size)).repeat_interleave(2, dim=1)
    key = F.pad(key, (0, 0, 0, pad_size)).repeat_interleave(2, dim=1)
    value = F.pad(value, (0, 0, 0, pad_size))
    beta = F.pad(beta, (0, pad_size))
    g = F.pad(g, (0, pad_size))
    tot_heads = num_heads + pad_size
    scale = 1 / (query.shape[-1]**0.5)
    query = query * scale

    v_beta = value * beta.unsqueeze(-1)
    k_beta = key * beta.unsqueeze(-1)
    # reshape to chunks
    query, key, value, k_beta, v_beta = [
        x.reshape(x.shape[0], x.shape[1], -1, chunk_size, x.shape[-1])
        for x in (query, key, value, k_beta, v_beta)
    ]
    g = g.reshape(g.shape[0], g.shape[1], -1, chunk_size)
    mask = torch.triu(torch.ones(chunk_size,
                                 chunk_size,
                                 dtype=torch.bool,
                                 device=query.device),
                      diagonal=0)

    # chunk decay
    g = g.cumsum(dim=-1)
    decay_mask = ((g.unsqueeze(-1) -
                   g.unsqueeze(-2)).tril().exp().float()).tril()
    attn = -(
        (k_beta @ key.transpose(-1, -2)) * decay_mask).masked_fill(mask, 0)
    for i in range(1, chunk_size):
        row = attn[..., i, :i].clone()
        sub = attn[..., :i, :i].clone()
        attn[..., i, :i] = row + (row.unsqueeze(-1) * sub).sum(-2)
    attn = attn + torch.eye(chunk_size, dtype=attn.dtype, device=attn.device)
    value = attn @ v_beta
    k_cumdecay = attn @ (k_beta * g.exp().unsqueeze(-1))

    last_recurrent_state = (torch.zeros(batch_size, sequence_length,
                                        k_head_dim, v_head_dim).to(value) if
                            initial_state is None else initial_state.to(value))

    core_attn_out = torch.zeros_like(value)
    mask = torch.triu(torch.ones(chunk_size,
                                 chunk_size,
                                 dtype=torch.bool,
                                 device=query.device),
                      diagonal=1)

    # for each chunk
    for i in range(0, tot_heads // chunk_size):
        q_i, k_i, v_i = query[:, :, i], key[:, :, i], value[:, :, i]
        attn = (q_i @ k_i.transpose(-1, -2) *
                decay_mask[:, :, i]).masked_fill_(mask, 0)
        v_prime = (k_cumdecay[:, :, i]) @ last_recurrent_state
        v_new = v_i - v_prime
        attn_inter = (q_i * g[:, :, i, :, None].exp()) @ last_recurrent_state
        core_attn_out[:, :, i] = attn_inter + attn @ v_new
        last_recurrent_state = (
            last_recurrent_state * g[:, :, i, -1, None, None].exp() +
            (k_i *
             (g[:, :, i, -1, None] - g[:, :, i]).exp()[..., None]).transpose(
                 -1, -2) @ v_new)

    if not output_final_state:
        last_recurrent_state = None
    core_attn_out = core_attn_out.reshape(core_attn_out.shape[0],
                                          core_attn_out.shape[1], -1,
                                          core_attn_out.shape[-1])
    core_attn_out = core_attn_out[:, :, :num_heads]
    core_attn_out = core_attn_out.transpose(1,
                                            2).contiguous().to(initial_dtype)
    return core_attn_out, last_recurrent_state

@triton.heuristics(
    {
        "USE_INITIAL_STATE": lambda args: args["h0_source"] is not None,
        "IS_VARLEN": lambda args: args["cu_seqlens"] is not None,
    }
)
@triton.jit(do_not_specialize=["T"])
def fused_sigmoid_gating_delta_rule_update_kernel(
    A_log,
    a,
    dt_bias,
    softplus_beta,
    softplus_threshold,
    q,
    k,
    v,
    b,
    o,
    h0_source,
    h0_indices,
    cu_seqlens,
    scale,
    T,
    B: tl.constexpr,
    H: tl.constexpr,
    HV: tl.constexpr,
    K: tl.constexpr,
    V: tl.constexpr,
    BK: tl.constexpr,
    BV: tl.constexpr,
    USE_INITIAL_STATE: tl.constexpr,
    USE_QK_L2NORM_IN_KERNEL: tl.constexpr,
    IS_VARLEN: tl.constexpr,
):
    """
    Fused kernel that combines sigmoid gating computation with recurrent delta rule update.
    """
    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)
    i_n, i_hv = i_nh // HV, i_nh % HV
    i_h = i_hv // (HV // H)

    if IS_VARLEN:
        bos, eos = (
            tl.load(cu_seqlens + i_n).to(tl.int64),
            tl.load(cu_seqlens + i_n + 1).to(tl.int64),
        )
        all = T
        T = eos - bos
    else:
        bos, eos = i_n * T, i_n * T + T
        all = B * T

    o_k = i_k * BK + tl.arange(0, BK)
    o_v = i_v * BV + tl.arange(0, BV)

    p_q = q + (bos * H + i_h) * K + o_k
    p_k = k + (bos * H + i_h) * K + o_k
    p_v = v + (bos * HV + i_hv) * V + o_v
    p_b = b + bos * HV + i_hv
    p_o = o + ((i_k * all + bos) * HV + i_hv) * V + o_v

    # Gating computation pointers
    p_A_log = A_log + i_hv
    p_a = a + bos * HV + i_hv
    p_dt_bias = dt_bias + i_hv

    mask_k = o_k < K
    mask_v = o_v < V
    mask_h = mask_k[:, None] & mask_v[None, :]

    b_h = tl.zeros([BK, BV], dtype=tl.float32)
    if USE_INITIAL_STATE:
        idx = tl.load(h0_indices + i_n)
        # if idx >= 0:
        tmp0 = tl.where(idx < 0, 0, idx)
        p_h0 = (
            h0_source
            + tmp0 * HV * K * V
            + i_hv * K * V
            + o_k[:, None] * V
            + o_v[None, :]
        )
        temp1 = tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)
        temp2 = tl.zeros_like(temp1)
        value0 = tl.where(idx < 0, temp2, temp1)
        b_h += value0  # tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)

    for i in range(0, T):
        # Load inputs
        b_q = tl.load(p_q + i * H * K, mask=mask_k, other=0).to(tl.float32)
        b_k = tl.load(p_k + i * H * K, mask=mask_k, other=0).to(tl.float32)
        b_v = tl.load(p_v + i * HV * V, mask=mask_v, other=0).to(tl.float32)
        b_b = tl.load(p_b + i * HV).to(tl.float32)

        # Compute sigmoid gating
        # Load gating parameters
        b_A_log = tl.load(p_A_log).to(tl.float32)
        b_a = tl.load(p_a + i * HV).to(tl.float32)
        b_dt_bias = tl.load(p_dt_bias).to(tl.float32)

        # Compute g = -exp(A_log) * softplus(a + dt_bias)
        x = b_a + b_dt_bias
        beta_x = softplus_beta * x
        # Apply softplus with numerical stability
        softplus_x = tl.where(
            beta_x <= softplus_threshold,
            (1.0 / softplus_beta) * tl.log(1.0 + tl.exp(beta_x)),
            x,
        )
        b_g = -tl.exp(b_A_log) * softplus_x

        # Compute beta = sigmoid(b)
        b_beta = 1.0 / (1.0 + tl.exp(-b_b))

        # Apply L2 normalization if enabled
        if USE_QK_L2NORM_IN_KERNEL:
            b_q = b_q / (tl.sqrt(tl.sum(b_q * b_q)) + 1e-6)
            b_k = b_k / (tl.sqrt(tl.sum(b_k * b_k)) + 1e-6)

        b_q = b_q * scale

        # Apply gating to hidden state: h *= exp(g)
        b_h *= tl.exp(b_g)

        # Delta rule: v -= sum(h * k, dim=0)
        b_v -= tl.sum(b_h * b_k[:, None], 0)

        # Apply beta gating: v *= beta
        b_v *= b_beta

        # Update hidden state: h += k[:, None] * v[None, :]
        b_h += b_k[:, None] * b_v[None, :]

        # Compute output: o = sum(h * q, dim=0)
        b_o = tl.sum(b_h * b_q[:, None], 0)
        tl.store(p_o + i * HV * V, b_o.to(p_o.dtype.element_ty), mask=mask_v)

        # # Update pointers for next timestep
        # p_q += H * K
        # p_k += H * K
        # p_o += HV * V
        # p_v += HV * V
        # p_b += HV
        # p_a += HV

    # Store final state back to h0_source with bounds checking
    if USE_INITIAL_STATE:
        idx = tl.load(h0_indices + i_n)
        if idx >= 0:
            p_h0 = (
                h0_source
                + idx * HV * K * V
                + i_hv * K * V
                + o_k[:, None] * V
                + o_v[None, :]
            )
            tl.store(p_h0, b_h.to(p_h0.dtype.element_ty), mask=mask_h)


def fused_sigmoid_gating_delta_rule_update(
    A_log: torch.Tensor,
    a: torch.Tensor,
    dt_bias: torch.Tensor,
    softplus_beta: float,
    softplus_threshold: float,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    b: torch.Tensor,
    initial_state_source: torch.Tensor,
    initial_state_indices: torch.Tensor,
    scale: Optional[float] = None,
    use_qk_l2norm_in_kernel: bool = False,
    cu_seqlens: Optional[torch.Tensor] = None,
):
    """
    Fused triton implementation of sigmoid gating delta rule update.
    This function uses a single fused kernel that combines both sigmoid gating computation
    and the recurrent delta rule update for better performance.
    """
    B, T, H, K, V = *k.shape, v.shape[-1]
    HV = v.shape[2]
    N = B if cu_seqlens is None else len(cu_seqlens) - 1
    BK, BV = triton.next_power_of_2(K), min(triton.next_power_of_2(V), 64)
    NK, NV = triton.cdiv(K, BK), triton.cdiv(V, BV)
    assert NK == 1, "NK > 1 is not supported yet"
    num_stages = 3
    num_warps = 1

    if scale is None:
        scale = k.shape[-1] ** -0.5
    else:
        assert scale > 0, "scale must be positive"

    o = q.new_empty(NK, *v.shape)
    grid = (NK, NV, N * HV)

    fused_sigmoid_gating_delta_rule_update_kernel[grid](
        A_log=A_log,
        a=a,
        dt_bias=dt_bias,
        softplus_beta=softplus_beta,
        softplus_threshold=softplus_threshold,
        q=q,
        k=k,
        v=v,
        b=b,
        o=o,
        h0_source=initial_state_source,
        h0_indices=initial_state_indices,
        cu_seqlens=cu_seqlens,
        scale=scale,
        T=T,
        B=B,
        H=H,
        HV=HV,
        K=K,
        V=V,
        BK=BK,
        BV=BV,
        USE_QK_L2NORM_IN_KERNEL=use_qk_l2norm_in_kernel,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    o = o.squeeze(0)
    return o


# 替换layer norm func
class LayerNormFn(torch.autograd.Function):
    @staticmethod
    def forward(ctx,
                x,
                weight,
                bias,
                z=None,
                eps=1e-6,
                group_size=None,
                norm_before_gate=True,
                is_rms_norm=False):
        """If z is not None, we do norm(x) * silu(z) if norm_before_gate, else norm(x * silu(z))
        """

        x_shape_og = x.shape
        # reshape input data into 2D tensor
        x = x.reshape(-1, x.shape[-1])
        if x.stride(-1) != 1:
            x = x.contiguous()
        if z is not None:
            assert z.shape == x_shape_og
            z = z.reshape(-1, z.shape[-1])
            if z.stride(-1) != 1:
                z = z.contiguous()
        weight = weight.contiguous()
        if bias is not None:
            bias = bias.contiguous()
        y, mean, rstd = layer_norm_fwd(
            x,
            weight,
            bias,
            eps,
            z=z,
            group_size=group_size,
            norm_before_gate=norm_before_gate,
            is_rms_norm=is_rms_norm,
        )
        ctx.save_for_backward(x, weight, bias, mean, rstd, z)
        ctx.x_shape_og = x_shape_og
        ctx.eps = eps
        ctx.group_size = group_size
        ctx.norm_before_gate = norm_before_gate
        ctx.is_rms_norm = is_rms_norm
        return y.reshape(x_shape_og)
    

# 替换之后的forward方法
def layer_norm_fwd(
    x: torch.Tensor,
    weight: torch.Tensor,
    bias: torch.Tensor,
    eps: float,
    z: torch.Tensor = None,
    out: torch.Tensor = None,
    group_size: int = None,
    norm_before_gate: bool = True,
    is_rms_norm: bool = False,
):
    M, N = x.shape
    if group_size is None:
        group_size = N
    assert N % group_size == 0
    ngroups = N // group_size
    assert x.stride(-1) == 1
    if z is not None:
        assert z.stride(-1) == 1
        assert z.shape == (M, N)
    assert weight.shape == (N, )
    assert weight.stride(-1) == 1
    if bias is not None:
        assert bias.stride(-1) == 1
        assert bias.shape == (N, )
    # allocate output
    if out is not None:
        assert out.shape == x.shape
    else:
        out = torch.empty_like(x)
    assert out.stride(-1) == 1
    mean = torch.empty((ngroups * M, ), dtype=torch.float32,
                       device=x.device) if not is_rms_norm else None
    rstd = torch.empty((ngroups * M, ), dtype=torch.float32, device=x.device)
    # Less than 64KB per feature: enqueue fused kernel
    MAX_FUSED_SIZE = 65536 // x.element_size()

    BLOCK_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(group_size))
    if group_size > BLOCK_N:
        raise RuntimeError(
            "This layer norm doesn't support feature dim >= 64KB.")

    # SUB_BLOCK_M = 128 # 1835008 when N = 128, bf16 (overflow) -> assume factor 56
    # Assume large M
    SUB_BLOCK_M = triton.next_power_of_2(triton.cdiv(1572864, 56 * N) // x.element_size()) // 2
    import triton.runtime.driver as driver
    device = torch.npu.current_device()
    num_cores = driver.active.utils.get_device_properties(device)["num_vectorcore"]
    BLOCK_M = max(triton.next_power_of_2(triton.cdiv(triton.cdiv(M, SUB_BLOCK_M), num_cores)) * SUB_BLOCK_M, SUB_BLOCK_M)

    # heuristics for number of warps
    num_warps = min(max(BLOCK_N // 256, 1), 8)
    grid = (triton.cdiv(M, BLOCK_M), ngroups)
    layer_norm_fwd_kernel[grid](x,
                                out,
                                weight,
                                bias,
                                z,
                                mean,
                                rstd,
                                x.stride(0),
                                out.stride(0),
                                z.stride(0) if z is not None else 0,
                                M,
                                group_size,
                                eps,
                                BLOCK_N=BLOCK_N,
                                BLOCK_M=BLOCK_M,
                                SUB_BLOCK_M=SUB_BLOCK_M,
                                HAS_BIAS=bias is not None,
                                HAS_Z=z is not None,
                                NORM_BEFORE_GATE=norm_before_gate,
                                IS_RMS_NORM=is_rms_norm,
                                num_warps=num_warps)
    return out, mean, rstd


# 替换之后的算子
@triton.heuristics({
    "HAS_BIAS": lambda args: args["B"] is not None,
    "HAS_Z": lambda args: args["Z"] is not None,
})
@triton.jit
def layer_norm_fwd_kernel(
    X,  # pointer to the input
    Y,  # pointer to the output
    W,  # pointer to the weights
    B,  # pointer to the biases
    Z,  # pointer to the other branch
    Mean,  # pointer to the mean
    Rstd,  # pointer to the 1/std
    stride_x_row,  # how much to increase the pointer when moving by 1 row
    stride_y_row,
    stride_z_row,
    M,  # number of rows in X
    N,  # number of columns in X
    eps,  # epsilon to avoid division by zero
    BLOCK_N: tl.constexpr,
    BLOCK_M: tl.constexpr,
    SUB_BLOCK_M: tl.constexpr,
    HAS_BIAS: tl.constexpr,
    HAS_Z: tl.constexpr,
    NORM_BEFORE_GATE: tl.constexpr,
    IS_RMS_NORM: tl.constexpr,
):
    # Map the program id to the row of X and Y it should compute.
    row_blk_idx = tl.program_id(0)
    group = tl.program_id(1)

    cols = tl.arange(0, BLOCK_N)
    col_mask = cols < N
    W_inner = W + group * N + cols
    if HAS_BIAS:
        B_inner = B + group * N + cols

    w = tl.load(W_inner, mask=col_mask).to(tl.float32)
    if HAS_BIAS:
        b = tl.load(B_inner, mask=col_mask).to(tl.float32)

    for sub_row_blk in range(0, BLOCK_M // SUB_BLOCK_M):
        row = row_blk_idx * BLOCK_M + sub_row_blk * SUB_BLOCK_M + tl.arange(0, SUB_BLOCK_M)
        row_mask = row < M
        # cols = tl.arange(0, BLOCK_N)
        # col_mask = cols < N
        blk_mask = row_mask[:, None] & col_mask[None, :]

        X_inner = X + row[:, None] * stride_x_row + (group * N + cols)[None, :]
        Y_inner = Y + row[:, None] * stride_y_row + (group * N + cols)[None, :]
        if HAS_Z:
            Z_inner = Z + row[:, None] * stride_z_row + (group * N + cols)[None, :]
        if not IS_RMS_NORM:
            Mean_inner = Mean + group * M + row[:, None]
        Rstd_inner = Rstd + group * M + row[:, None]
        # W_inner = W + group * N + cols
        # if HAS_BIAS:
        #     B_inner = B + group * N + cols
        # Compute mean and variance

        x = tl.load(X_inner, mask=blk_mask, other=0.).to(tl.float32)

        if HAS_Z and not NORM_BEFORE_GATE:
            z = tl.load(Z_inner, mask=blk_mask).to(tl.float32)
            x *= z * tl.sigmoid(z)

        if not IS_RMS_NORM:
            mean = tl.sum(x, axis=1, keep_dims=True) / N
            tl.store(Mean_inner, mean, mask=row_mask[:, None])
            # xbar = tl.where(cols < N, x - mean, 0.)
            xbar = x - mean
            var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / N
        else:
            # xbar = tl.where(cols < N, x, 0.)
            xbar = x
            var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / N

        rstd = 1 / tl.sqrt(var + eps)
        tl.store(Rstd_inner, rstd, mask=row_mask[:, None])
        # Normalize and apply linear transformation

        x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd

        y = x_hat * w + b if HAS_BIAS else x_hat * w

        if HAS_Z and NORM_BEFORE_GATE:
            z = tl.load(Z_inner, mask=blk_mask).to(tl.float32)
            y *= z * tl.sigmoid(z)
        # Write output
        tl.store(Y_inner, y, mask=blk_mask)

def get_npu_properties():
    device = torch.npu.current_device()
    return driver.active.utils.get_device_properties(device)

@triton.autotune(
    configs=[
        triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]
    ],
    key=["D"],
)
@triton.jit
def l2norm_fwd_kernel1(
    x,
    y,
    D,
    BD: tl.constexpr,
    eps,
):
    i_t = tl.program_id(0)
    x += i_t * D
    y += i_t * D
    # Compute mean and variance
    cols = tl.arange(0, BD)
    mask = cols < D
    b_x = tl.load(x + cols, mask=mask, other=0.0).to(tl.float32)
    b_var = tl.sum(b_x * b_x, axis=0)
    b_rstd = 1 / tl.sqrt(b_var + eps)
    # tl.store(Rstd + i_t, rstd)
    # Normalize and apply linear transformation
    b_y = b_x * b_rstd
    tl.store(y + cols, b_y, mask=mask)


@triton.autotune(
    configs=[
        triton.Config({"BT": BT}, num_warps=num_warps)
        for num_warps in [1, 2, 4, 8, 16]
        for BT in [8, 16, 32, 64, 128]
    ],
    key=["D"],
)
@triton.jit(do_not_specialize=["NB"])
def l2norm_fwd_kernel(
    x,
    y,
    eps,
    NB,
    T,
    D: tl.constexpr,
    BT: tl.constexpr,
    BD: tl.constexpr,
):
    i_t = tl.program_id(0)
    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))
    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)
    b_var = tl.sum(b_x * b_x, axis=1)
    b_y = b_x / tl.sqrt(b_var + eps)[:, None]
    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))
    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))


@triton.jit
def l2norm_fwd_kernel2_loop(X, Y, eps, M, N: tl.constexpr,
                           MBLOCK: tl.constexpr, NUM_CHUNKS: tl.constexpr):
    # 一个核负责 NUM_CHUNKS * MBLOCK 行
    base_row = tl.program_id(0) * (NUM_CHUNKS * MBLOCK)
    rindex = tl.arange(0, N)[None, :]

    # 循环分批处理
    for chunk in range(NUM_CHUNKS):
        row_idx = base_row + chunk * MBLOCK + tl.arange(0, MBLOCK)[:, None]
        xmask = row_idx < M

        xs = tl.load(X + (rindex + N * row_idx), xmask).to(tl.float32)
        square = xs * xs
        square_sum = tl.sum(tl.where(xmask, square, 0), 1)[:, None]
        rsqrt = tl.rsqrt(square_sum + eps)

        tl.store(Y + (rindex + N * row_idx), xs * rsqrt, xmask)



# 把这个算子替换进torch_chunk_gated_delta_rule
def l2norm_fwd_new(
    x: torch.Tensor, eps: float = 1e-6, output_dtype: torch.dtype | None = None
):
    x_shape_og = x.shape
    x = x.reshape(-1, x.shape[-1])
    # allocate output
    if output_dtype is None:
        y = torch.empty_like(x)
    else:
        y = torch.empty_like(x, dtype=output_dtype)
    assert y.stride(-1) == 1
    T, D = x.shape[0], x.shape[-1]
    # rstd = torch.empty((T,), dtype=torch.float32, device=x.device)
    # Less than 64KB per feature: enqueue fused kernel
    MAX_FUSED_SIZE = 65536 // x.element_size()
    BD = min(MAX_FUSED_SIZE, triton.next_power_of_2(D))
    if D > BD:
        raise RuntimeError("This layer doesn't support feature dim >= 64KB.")

    if not USE_DEFAULT_FLA_NORM:
        MBLOCK = 69
        # M, N = x.shape
        num_core = get_npu_properties()["num_vectorcore"]
        main_bs = triton.cdiv(T, num_core)
        num_sub_blocks = triton.cdiv(main_bs, MBLOCK)
        grid = (num_core, )
        l2norm_fwd_kernel2_loop[grid](
            X=x,
            Y=y,
            eps=eps,
            M=T,
            N=D,
            MBLOCK=MBLOCK,
            NUM_CHUNKS=num_sub_blocks,
        )
    else:
        if D <= 512:
            NB = triton.cdiv(T, 2048)

            def grid(meta):
                return (triton.cdiv(T, meta["BT"]),)

            l2norm_fwd_kernel[grid](
                x,
                y,
                eps,
                NB=NB,
                T=T,
                D=D,
                BD=BD,
            )
        else:
            l2norm_fwd_kernel1[(T,)](
                x,
                y,
                eps=eps,
                D=D,
                BD=BD,
            )

    return y.view(x_shape_og)
